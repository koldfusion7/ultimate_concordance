# Ultimate Hebrew & Aramaic Concordance Project

This repository serves as the foundation for an unabridged concordance and dictionary of **Biblical Hebrew**, **Biblical and Targumic Aramaic**, and **Modern Hebrew**.  It is designed to support the creation of an exhaustive dataset that links every lexical entry to every place it appears across the **Tanakh**, the **Targums**, and the **Peshitta**.  Additionally, the dataset will integrate a modern Hebrew dictionary so that one project covers the entire diachronic breadth of Hebrew.

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ lexicon_entries.json           # Master list of lexical entries across all corpora
â”‚Â Â  â”œâ”€â”€ tanakh_concordance.json        # Concordance linking Tanakh verses to lexical entries
â”‚Â Â  â”œâ”€â”€ targums_concordance.json       # Concordance linking Targum verses to lexical entries
â”‚Â Â  â”œâ”€â”€ peshitta_concordance.json      # Concordance linking Peshitta verses to lexical entries
â”‚Â Â  â”œâ”€â”€ modern_hebrew_dictionary.json  # Modern Hebrew dictionary entries
â”‚Â Â  â””â”€â”€ sources/                       # Place holders for raw sources (PDFs, modules)
â”‚       â”œâ”€â”€ 00_README.md               # Guidelines on how to populate `sources`
â”‚       â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚Â Â  â”œâ”€â”€ parse_pdf.py                   # Parses PDF lexica into structured JSON entries
â”‚Â Â  â”œâ”€â”€ parse_esword.py                # Parses eâ€‘Sword modules (BBLX/BBLI, DCTX/DCTI, etc.) into JSON
â”‚Â Â  â”œâ”€â”€ create_concordance.py          # Generates concordance files for Tanakh/Targums/Peshitta
â”‚Â Â  â””â”€â”€ utils.py                       # Shared helper functions (tokenization, transliteration, etc.)
â”œâ”€â”€ docs/
â”‚Â Â  â””â”€â”€ schema.md                      # Detailed specification of the JSON structures
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                          # Highâ€‘level overview and project goals
```

### `data/`

All produced datasets live in this directory.  JSON is used because it is machineâ€‘readable and integrates cleanly with modern programming environments.  Each entry in the concordance files will store references (book, chapter, verse, and word position) along with pointers to a lexical entry in `lexicon_entries.json`.

The `sources/` subdirectory **must not** be versioned with proprietary or copyrighted texts (for example, commercially licensed PDFs or premium eâ€‘Sword modules).  Instead, this directory is where you drop your input files locally.  The parser scripts will read from here and produce JSON files in `../`.

### `scripts/`

This folder houses all the Python scripts used to convert raw input into the unified dataset:

* `parse_pdf.py` â€“ Accepts a PDF (e.g., a scanned lexicon) and extracts entries into a structured JSON file.  It uses `pdfminer.six` for text extraction and expects that the PDF uses a consistent heading format (see `docs/schema.md` for details).
* `parse_esword.py` â€“ Reads eâ€‘Sword modules.  eâ€‘Sword v9â€“10 modules (extensions `.bblx`, `.cmtx`, `.dctx`, `.topx`, etc.) are SQLite databases containing RTF strings.  This script uses Pythonâ€™s `sqlite3` module and the `rtf_to_text` helper in `utils.py` to extract plain text.  For v11+ modules (extensions `.bbli`, `.cmti`, `.dcti`, `.refi`, etc.), the contents are stored as HTML.  The script uses `html2text` to strip markup.
* `create_concordance.py` â€“ Once lexical entries are available and the Biblical corpora are in machineâ€‘readable form (USFM, OSIS, etc.), this script tokenizes each verse, normalizes each word (with appropriate lemmatization), and records references back to the lexicon.
* `utils.py` â€“ Shared functionality (e.g., Unicode normalization, transliteration helpers, gematria calculation stubs).

## ğŸš€ Getting Started

1. **Clone the repository:**

   ```bash
   git clone <yourâ€‘forkâ€‘url>
   cd ultimate_concordance
   ```

2. **Install dependencies:**  The scripts rely on Python 3.9+ and some external libraries (see `requirements.txt`).  You can install them with:

   ```bash
   pip install -r requirements.txt
   ```

3. **Place source files:**  Put your PDF lexicon(s), eâ€‘Sword modules, and any other raw sources into `data/sources/`.  The names of these files must be referenced when you run the scripts.  Do **not** commit those proprietary sources to version control.

4. **Run the parsers:**  Examples:

   ```bash
   # Convert a lexicon PDF into JSON
   python scripts/parse_pdf.py --input data/sources/my_lexicon.pdf --output data/lexicon_entries.json

   # Convert eâ€‘Sword dictionary modules into JSON
   python scripts/parse_esword.py --input data/sources/my_dictionary.dctx --output data/modern_hebrew_dictionary.json

   # Generate concordances once all corpora are available as plain text
   python scripts/create_concordance.py --tanakh data/sources/tanakh.usfm --targums data/sources/targum.osis --peshitta data/sources/peshitta.usfm
   ```

5. **Review and iterate:**  Inspect the generated JSON files to ensure the schema meets your needs.  Contributions and improvements are welcome!

## ğŸ“– Schema Overview

See `docs/schema.md` for a detailed specification of the JSON formats used throughout the project.  Each lexical entry includes fields such as `lemma`, `language` (Hebrew or Aramaic), `definitions`, `pos` (part of speech), `etymology`, and `modern_equivalent` when applicable.  Concordance entries consist of `lemma_id` (foreign key to the lexicon), `source` (Tanakh, Targum, Peshitta), `reference` (book, chapter, verse), and `occurrence_indices` (0â€‘based positions of the lemma within the verse).

## âš ï¸ Licensing and Copyright

This repository itself only contains **code** and **open data** (e.g., schema definitions, demonstration snippets).  When working with copyrighted sources (commercial dictionaries or translations), you must obtain proper permission before including them in a public dataset.  The parsers are provided as a tool for **personal or scholarly use** and are not meant to distribute proprietary content.

## ğŸ› ï¸ Future Work

* Implement gematria calculation methods in `utils.py` (Hebrew ordinal, standard, reduced, Atbash, etc.), and integrate them into the lexical entries.
* Add a web UI for browsing the concordance and dictionary.
* Expand `parse_pdf.py` to handle OCR of scanned documents via `pytesseract` when text extraction fails.

---

ğŸ™ **Thank you for contributing to this important project.**  May this work serve as a valuable resource for scholars and students seeking to study the Tanakh, Targums, Peshitta, and modern Hebrew in depth.